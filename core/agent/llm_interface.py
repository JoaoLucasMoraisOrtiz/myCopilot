"""
Agent LLM Interface Module

ResponsÃ¡vel pela comunicaÃ§Ã£o com o LLM e gestÃ£o de contexto.
"""

import os
from core.llm.llm_client import LLMClient
from devtools.gemini_client import GeminiLLMInterface
from devtools.gemini_api_client import GeminiAPILLMInterface
from devtools.codestral_api_client import CodestralLLMInterface


class AgentLLMInterface:
    """Classe responsÃ¡vel pela comunicaÃ§Ã£o e gestÃ£o de contexto com o LLM."""
    
    def __init__(self, llm_type="vscode", api_key=None):
        """
        Inicializa a interface do LLM.
        
        Args:
            llm_type: Tipo de LLM a usar ("vscode", "gemini", "gemini_api", ou "codestral")
            api_key: Chave da API (necessÃ¡ria para gemini_api e codestral)
        """
        self.llm_type = llm_type
        self.gemini_interface = None
        self.gemini_api_interface = None
        self.codestral_interface = None
        
        if llm_type == "gemini":
            # Inicializa interface Gemini via Chrome DevTools se especificado
            try:
                # URL padrÃ£o do Chrome DevTools (pode ser configurÃ¡vel)
                websocket_url = "ws://127.0.0.1:9222"
                self.gemini_interface = GeminiLLMInterface(websocket_url)
                print("ğŸ”— Interface Gemini (Chrome) inicializada")
            except Exception as e:
                print(f"âš ï¸ Erro ao inicializar Gemini Chrome, fallback para VS Code: {e}")
                self.llm_type = "vscode"
        elif llm_type == "gemini_api":
            # Inicializa interface Gemini via API se especificado
            try:
                self.gemini_api_interface = GeminiAPILLMInterface(api_key)
                print("ğŸ”— Interface Gemini (API) inicializada")
            except Exception as e:
                print(f"âš ï¸ Erro ao inicializar Gemini API, fallback para VS Code: {e}")
                self.llm_type = "vscode"
        elif llm_type == "codestral":
            # Inicializa interface Codestral via API se especificado
            try:
                self.codestral_interface = CodestralLLMInterface(api_key)
                print("ğŸ”— Interface Codestral (API) inicializada")
            except Exception as e:
                print(f"âš ï¸ Erro ao inicializar Codestral API, fallback para VS Code: {e}")
                self.llm_type = "vscode"
    
    def call_llm(self, messages: list, current_turn: int = 0) -> str:
        """
        Chama o LLM com gestÃ£o inteligente de contexto.
        
        Args:
            messages: Lista de mensagens da conversa
            current_turn: Turno atual para aplicar pressÃ£o temporal
            
        Returns:
            Resposta do LLM
        """
        # Limita o tamanho total do contexto para evitar overflow
        # Apenas as Ãºltimas mensagens sÃ£o enviadas para evitar duplicaÃ§Ã£o de contexto
        last_messages = messages[-4:]
        prompt = '\n'.join([f"{msg['role'].upper()}: {msg['content']}" for msg in last_messages])
        
        # Adiciona pressÃ£o temporal baseada no nÃºmero de turnos
        # pressure_message = self._get_pressure_message(current_turn)
        # if pressure_message:
        #     print(f"ğŸ¯ APLICANDO PRESSÃƒO TEMPORAL (Turno {current_turn}): {pressure_message[:80]}...")
        #     prompt += f"\n\nUSER: {pressure_message}"
        
        # Log do tamanho final do prompt
        print(f"ğŸ“Š Enviando prompt de {len(prompt)} chars para o LLM...")
        
        # Escolhe a interface baseada no tipo
        if self.llm_type == "gemini" and self.gemini_interface:
            try:
                response = self.gemini_interface.send_message(prompt)
                print(f"ğŸ’¬ Resposta do Gemini (Chrome) recebida: {len(response)} chars")
                return response
            except Exception as e:
                print(f"âš ï¸ Erro no Gemini Chrome, fallback para VS Code: {e}")
                # Fallback para VS Code em caso de erro
        elif self.llm_type == "gemini_api" and self.gemini_api_interface:
            try:
                # Converte o prompt para formato de mensagens se necessÃ¡rio
                if isinstance(messages, list) and len(messages) > 0:
                    response = self.gemini_api_interface.call_llm(messages)
                else:
                    # Fallback: converte prompt para mensagens
                    messages_fallback = [{"role": "user", "content": prompt}]
                    response = self.gemini_api_interface.call_llm(messages_fallback)
                print(f"ğŸ¤– Resposta do Gemini (API) recebida: {len(response)} chars")
                return response
            except Exception as e:
                print(f"âš ï¸ Erro no Gemini API, fallback para VS Code: {e}")
                # Fallback para VS Code em caso de erro
        elif self.llm_type == "codestral" and self.codestral_interface:
            try:
                # Converte o prompt para formato de mensagens se necessÃ¡rio
                if isinstance(messages, list) and len(messages) > 0:
                    response = self.codestral_interface.call_llm(messages)
                else:
                    # Fallback: converte prompt para mensagens
                    messages_fallback = [{"role": "user", "content": prompt}]
                    response = self.codestral_interface.call_llm(messages_fallback)
                print(f"ï¿½ Resposta do Codestral (API) recebida: {len(response)} chars")
                return response
            except Exception as e:
                print(f"âš ï¸ Erro no Codestral API, fallback para VS Code: {e}")
                # Fallback para VS Code em caso de erro
        
        # Interface padrÃ£o VS Code
        client = LLMClient()
        client.connect()
        response = client.send_prompt(prompt)
        client.close()
        
        # Log da resposta recebida
        if response:
            print(f"ğŸ“¨ Resposta recebida: {len(response)} chars")
            if len(response) > 20000:
                print("âš ï¸ Resposta muito grande detectada - processamento otimizado serÃ¡ aplicado")
        
        return response
    
    def _get_pressure_message(self, current_turn: int) -> str:
        """Gera mensagens de pressÃ£o temporal baseadas no nÃºmero de turnos."""
        
        if current_turn >= 8:
            return ("URGENTE: VocÃª estÃ¡ no turno 8/10. DEVE fornecer final_answer no PRÃ“XIMO turno "
                   "ou perderÃ¡ a oportunidade de responder. Analise se jÃ¡ tem informaÃ§Ã£o suficiente "
                   "para uma resposta Ãºtil.")
        elif current_turn >= 6:
            return ("ATENÃ‡ÃƒO: VocÃª estÃ¡ no turno 6/10. Comece a considerar seriamente usar final_answer. "
                   "VocÃª tem informaÃ§Ã£o suficiente para uma resposta parcial Ãºtil?")
        elif current_turn >= 5:
            return ("REFLEXÃƒO: JÃ¡ coletou bastante informaÃ§Ã£o. Avalie se pode fornecer uma resposta Ãºtil "
                   "ou se precisa de apenas mais 1-2 observaÃ§Ãµes especÃ­ficas.")
        elif current_turn >= 4:
            return ("FOCO: VocÃª estÃ¡ na metade do caminho. Mantenha o foco no objetivo principal "
                   "e evite exploraÃ§Ãµes tangenciais.")
        else:
            return ""  # Sem pressÃ£o nos primeiros turnos
